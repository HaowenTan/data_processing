{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "4a1368fd",
   "metadata": {},
   "source": [
    "å®‰è£…ä¾èµ–\n",
    "\n",
    "conda create -n wiki python=3.10\n",
    "\n",
    "conda activate wiki\n",
    "\n",
    "pip install -r requirements.txt"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe357f0b",
   "metadata": {},
   "source": [
    "# æå–æ•°æ®"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "131d4fe9",
   "metadata": {},
   "source": [
    "### wikiextractorçš„å¥½å¤„\n",
    "- è‡ªåŠ¨è¯†åˆ«å¹¶æ¸…é™¤è¯­æ³•ç»“æ„ï¼ˆå¼•ç”¨ï¼›æ¨¡æ¿ã€æ–‡ä»¶ã€HTMLæ ‡ç­¾ã€è¡¨æ ¼ã€å›¾åƒå¼•ç”¨ç­‰ï¼‰ï¼Œåªä¿ç•™æ­£æ–‡å†…å®¹\n",
    "- åˆ†æ®µåˆç†ï¼Œä¿ç•™æ–‡ç« ç»“æ„ï¼Œæ¯ç¯‡æ–‡ç« ä¼šè¢«è§£æä¸ºï¼štitleï¼Œtextï¼Œå¹¶æŒ‰æ®µè½åˆ†å—ï¼Œä¾¿äºåç»­ä½¿ç”¨æˆ–ç²¾è°ƒã€‚\n",
    "- æ”¯æŒå¤§è§„æ¨¡å¹¶è¡Œ\n",
    "- å¤šè¯­è¨€æ”¯æŒ\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa724d79",
   "metadata": {},
   "outputs": [],
   "source": [
    "!wikiextractor ../zhwiki-20250601-pages-articles-multistream.xml.bz2 -o extracted --json"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f513939",
   "metadata": {},
   "source": [
    "# å¯å‘å¼è¿‡æ»¤\n",
    "\n",
    "ä¸ºäº†æå‡è¯­æ–™è´¨é‡ï¼Œæˆ‘ä»¬åœ¨é¢„å¤„ç†åçš„æ–‡æœ¬ä¸Šåº”ç”¨äº†ä¸€ç³»åˆ—å¯å‘å¼è§„åˆ™ï¼Œä¸»è¦é’ˆå¯¹ç»´åŸºæ•°æ®ä¸­çš„å†—ä½™ã€æ— æ•ˆæˆ–æ ¼å¼ä¸è§„èŒƒå†…å®¹è¿›è¡Œè¿‡æ»¤å’Œæ¸…æ´—ã€‚ä»¥ä¸‹æ˜¯å…·ä½“ç­–ç•¥ï¼š\n",
    "1. é•¿åº¦è¿‡æ»¤(å¤ªçŸ­å¯èƒ½ä¿¡æ¯ä¸è¶³)\n",
    "2. é‡å®šå‘è¿‡æ»¤\n",
    "3. è¯­è¨€è½¬æ¢ï¼ˆå¤„ç† Wikipedia çš„å¤šè¯­è¨€æ¨¡æ¿ -{zh-cn:...}-ï¼Œä»¥åŠåˆ©ç”¨openccåº“ç¹ä½“æ¢ç®€ä½“ï¼‰\n",
    "4. ç»“æ„å†—ä½™æ¸…ç†ï¼ˆæ¸…é™¤ç©ºæ‹¬å· (), ï¼ˆï¼Œï¼‰, ï¼ˆï¼Œï¼‰ç­‰æ ¼å¼æ®‹ç•™æˆ–é”™è¯¯ï¼‰\n",
    "5. æ— æ„ä¹‰æ ‡é¢˜è¿‡æ»¤ï¼ˆåˆ é™¤æ ‡é¢˜/åˆ—è¡¨ç±»çŸ­å¥ï¼šè‹¥è¯¥è¡Œä¸ºæ ‡é¢˜å¼å†…å®¹ï¼ˆå­—æ•°â‰¤ 15 ä¸æ„æˆå®Œæ•´å¥å­ï¼‰ï¼Œè®¤ä¸ºæ˜¯ç»“æ„å™ªå£°ï¼Œåˆ é™¤è¯¥è¡Œï¼‰\n",
    "6. è‹±æ–‡æ¯”ä¾‹è¿‡æ»¤ï¼ˆå¯¹äºè‹±æ–‡å­—ç¬¦æ˜æ˜¾è¶…è¿‡ä¸­æ–‡å­—ç¬¦çš„å¥å­ï¼Œå¯è·³è¿‡ä¿ç•™ä¸­æ–‡ä¸»å¹²ä¸ºä¸»ï¼‰\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3106ce4b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# clean_and_convert.py\n",
    "import os\n",
    "import json\n",
    "import jsonlines\n",
    "import re\n",
    "from tqdm import tqdm\n",
    "from opencc import OpenCC\n",
    "\n",
    "cc = OpenCC('t2s')\n",
    "\n",
    "def is_valid_text(text):\n",
    "    \"\"\"è¿‡æ»¤æ— æ•ˆå†…å®¹ï¼ˆå¤ªçŸ­ã€å«ç‰¹æ®Šè¯ã€é‡å®šå‘ï¼‰\"\"\"\n",
    "    if len(text.strip()) < 200 or len(text.strip()) >8000:\n",
    "        return False\n",
    "    if '#REDIRECT' in text or '#é‡å®šå‘' in text:\n",
    "        return False\n",
    "    return True\n",
    "\n",
    "def clean_text(text):\n",
    "    \"\"\"ç®€å•æ¸…æ´—æ¨¡æ¿/å¼•ç”¨/åˆ†ç±»\"\"\"\n",
    "    #text = re.sub(r'\\[\\[Category:[^\\]]+\\]\\]', '', text)  # åˆ é™¤åˆ†ç±»\n",
    "    #text = re.sub(r'<ref[^<]*</ref>', '', text)          # åˆ é™¤refå¼•ç”¨\n",
    "    #text = re.sub(r'{{[^{}]+}}', '', text)               # åˆ é™¤æ¨¡æ¿\n",
    "    #text = re.sub(r'==+[^=]+==+', '', text)              # åˆ é™¤æ ‡é¢˜\n",
    "    text = re.sub(r'\\n+', '\\n', text).strip()            # å¤šä¸ªæ¢è¡Œåˆå¹¶\n",
    "    text = cc.convert(text)\n",
    "    return text\n",
    "\n",
    "\n",
    "def remove_bad_parentheses(text):\n",
    "    def should_remove(content):\n",
    "        # è¶…è¿‡ä¸€åŠæ˜¯è‹±æ–‡å­—æ¯æˆ–ç‰¹æ®Šç¬¦å·\n",
    "        english = len(re.findall(r'[A-Za-z]', content))\n",
    "        chinese = len(re.findall(r'[\\u4e00-\\u9fff]', content))\n",
    "        symbols = len(re.findall(r'[\\W_]', content))  # éå­—æ¯æ•°å­—\n",
    "        total = max(len(content), 1)\n",
    "\n",
    "        english_ratio = english / total\n",
    "        symbol_ratio = symbols / total\n",
    "\n",
    "        # æ˜æ˜¾æ˜¯æ‹‰ä¸æ–‡åæˆ–ç‰¹æ®Šè¯­ç§\n",
    "        bad_keywords = ['å­¦å', 'æ‹‰ä¸', 'è‹±æ–‡', 'è‹±è¯­', 'å¾·è¯­', 'Latin', 'åç§°','æ—§ç§°','è¯‘å','æ¸¯è¯‘','åˆè¯‘','æ—¥è¯­','æ—¥æ–‡']\n",
    "\n",
    "        if english_ratio > 0.7 and chinese == 0:\n",
    "            return True\n",
    "        if symbol_ratio > 0.4:\n",
    "            return True\n",
    "        if any(kw in content for kw in bad_keywords):\n",
    "            return True\n",
    "        return False\n",
    "\n",
    "    # åŒ¹é…ä¸­è‹±æ–‡æ‹¬å·å¯¹\n",
    "    return re.sub(\n",
    "        r'ï¼ˆ([^ï¼ˆï¼‰]{0,30})ï¼‰|[(ï¼ˆ]([^()ï¼ˆï¼‰]{0,30})[)ï¼‰]',\n",
    "        lambda m: (\n",
    "            '' if should_remove((m.group(1) or '') + (m.group(2) or '')) else m.group(0)\n",
    "        ),\n",
    "        text\n",
    "    )\n",
    "\n",
    "def clean_text_2(text: str) -> str:\n",
    "    # 1. å¤„ç†è¯­è¨€é€‰æ‹©è¯­æ³•ï¼šä¿ç•™ zh-cn éƒ¨åˆ†\n",
    "    text = re.sub(r'-\\{[^{}]*?zh-cn:([^;{}]+?)(;[^{}]*)?\\}-', r'\\1', text)\n",
    "    text = re.sub(r'-\\{[^{}]*?zh-hans:([^;{}]+?)(;[^{}]*)?\\}-', r'\\1', text)\n",
    "\n",
    "    # 2. æ‹¬å·å†…è‹±æ–‡æ‹‰ä¸ä¹±ç å¤„ç†\n",
    "    text = remove_bad_parentheses(text)\n",
    "    # å»æ‰æ‹¬å·å†…è¯­è¨€æ ‡æ³¨ç±»å†…å®¹ï¼Œå¦‚â€œå¾·è¯­: xxxâ€ã€â€œè‹±è¯­: xxxâ€ã€â€œæ³•è¯­: xxxâ€\n",
    "    text = re.sub(r'[ï¼ˆ(](å¾·è¯­|è‹±è¯­|æ³•è¯­|ä¿„è¯­|æ—¥è¯­|éŸ©è¯­|è¥¿ç­ç‰™è¯­|æ‹‰ä¸è¯­|è·å…°è¯­|æ·å…‹è¯­|åœŸè€³å…¶è¯­|æ„å¤§åˆ©è¯­|è‘¡è„ç‰™è¯­|åŒˆç‰™åˆ©è¯­|èŠ¬å…°è¯­|ä¹Œå…‹å…°è¯­|ä¿åŠ åˆ©äºšè¯­|å¸Œè…Šè¯­|ç‘å…¸è¯­|ä¸¹éº¦è¯­|æŒªå¨è¯­|ç½—é©¬å°¼äºšè¯­|æ–¯æ´›æ–‡å°¼äºšè¯­|çˆ±å°”å…°è¯­|æ³¢å…°è¯­|é˜¿æ‹‰ä¼¯è¯­|å¸Œä¼¯æ¥è¯­|ä¸–ç•Œè¯­)[ï¼š:][^ï¼‰)]{1,50}[ï¼‰)]', '', text)\n",
    "    # å»é™¤æ‹¬å·å†…ä»¥æ ‡ç‚¹å¼€å¤´çš„å†…å®¹ï¼ˆå¦‚\"ï¼ˆï¼Œç¼©å†™ï¼šEUVEï¼‰\"ï¼‰\n",
    "    text = re.sub(r'[ï¼ˆ(][ï¼Œã€ã€‚ã€ï¼š:ï¼›;\\s\\'\"â€˜â€™â€œâ€`~^!@#$%^&*ï¼Œ.?!\\-+=<>â€¦Â·â•¯ï¿£_ï¼Ã—]{1}[^ï¼ˆï¼‰()]{0,50}[ï¼‰)]', '', text)\n",
    "\n",
    "    text = re.sub(r'\\((\\s*[ï¼Œã€,]?\\s*)\\)', '', text)\n",
    "    text = re.sub(r'ï¼ˆ\\s*[ï¼Œã€,]?\\s*ï¼‰', '', text)\n",
    "\n",
    "    # 3. åˆ é™¤æ— æ„ä¹‰çš„çŸ­æ ‡é¢˜è¡Œï¼ˆâ‰¤ 5å­—ï¼Œä¸”æ— æ ‡ç‚¹ï¼‰\n",
    "    cleaned_lines = []\n",
    "    for line in text.splitlines():\n",
    "        stripped = line.strip()\n",
    "        if len(stripped) <= 15:\n",
    "            continue  # è·³è¿‡è¿™ç±»çŸ­æ ‡é¢˜è¡Œ\n",
    "        cleaned_lines.append(line)\n",
    "    text = \"\\n\".join(cleaned_lines)\n",
    "\n",
    "    return text.strip()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a6574cce",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "def is_mostly_english(line: str, threshold: float = 0.7) -> bool:\n",
    "    english_chars = len(re.findall(r'[A-Za-z]', line))\n",
    "    chinese_chars = len(re.findall(r'[\\u4e00-\\u9fff]', line))\n",
    "    total = len(line.strip())\n",
    "\n",
    "    if total == 0:\n",
    "        return False\n",
    "\n",
    "    ratio = english_chars / (total + 1e-5)\n",
    "    if english_chars > chinese_chars * 2 or ratio > threshold:\n",
    "        return True\n",
    "    return False\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a5868c5",
   "metadata": {},
   "source": [
    "### è¿‡æ»¤å¯è§†åŒ–"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d33093bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "\n",
    "def inspect_cleaning(input_dir, max_docs=5):\n",
    "    count = 0\n",
    "\n",
    "    for dirpath, _, filenames in os.walk(input_dir):\n",
    "        print(filenames.sort())\n",
    "        for fname in filenames:\n",
    "            if count >= max_docs:\n",
    "                return\n",
    "            path = os.path.join(dirpath, fname)\n",
    "            \n",
    "            with open(path, 'r', encoding='utf-8') as f:\n",
    "                for line in f:\n",
    "                    if count >= max_docs:\n",
    "                        return\n",
    "                    data = json.loads(line)\n",
    "                    raw_text = data['text']\n",
    "                    title = data['title']\n",
    "                    id = data['id']\n",
    "                    cleaned_text = clean_text(raw_text)\n",
    "                    cleaned_text = clean_text_2(cleaned_text)\n",
    "\n",
    "                    print(\"=\" * 80)\n",
    "                    print(f\"ğŸ“„ æ–‡ä»¶å: {fname}\")\n",
    "                    print(title,id)\n",
    "                    print(f\"ğŸ§¾ åŸå§‹æ–‡æœ¬å‰ 300 å­—:\\n{raw_text[:]}\")\n",
    "                    print(\"-\" * 80)\n",
    "                    print(f\"âœ… æ¸…æ´—åæ–‡æœ¬å‰ 300 å­—:\\n{cleaned_text[:]}\")\n",
    "                    print(\"=\" * 80)\n",
    "\n",
    "                    #input(\"ğŸ” æŒ‰ Enter æŸ¥çœ‹ä¸‹ä¸€æ¡ï¼Œæˆ– Ctrl+C ä¸­æ­¢...\")\n",
    "                    count += 1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a22614d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "inspect_cleaning(\"extracted/AA/\", max_docs=5)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc5ed123",
   "metadata": {},
   "source": [
    "### ä¿å­˜jsonlæ–‡ä»¶"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a124b7c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def processing_and_save(input_dir, output_jsonl_path, max_docs=1000):\n",
    "    count = 0\n",
    "    with jsonlines.open(output_jsonl_path, 'w') as writer:\n",
    "        for dirpath, _, filenames in os.walk(input_dir):\n",
    "            filenames.sort()\n",
    "            for fname in filenames:\n",
    "                if count >= max_docs:\n",
    "                    return\n",
    "                path = os.path.join(dirpath, fname)\n",
    "                with open(path, 'r', encoding='utf-8') as f:\n",
    "                    for line in f:\n",
    "                        if count >= max_docs:\n",
    "                            return\n",
    "                        data = json.loads(line)\n",
    "                        raw_text = data['text']\n",
    "                        title = cc.convert(data.get('title', ''))\n",
    "                        id_ = data.get('id', '')\n",
    "\n",
    "                        cleaned = clean_text(raw_text)\n",
    "                        cleaned = clean_text_2(cleaned)\n",
    "                        if not is_valid_text(cleaned):\n",
    "                            continue\n",
    "\n",
    "                        # ä¿å­˜ä¸ºå¯¹æ¯” jsonl æ–‡ä»¶\n",
    "                        writer.write({\"text\":cleaned.strip(),\n",
    "                            'meta':{\"id\": id_,\n",
    "                                              \"source\": \"zhwiki\",\n",
    "                                              'language':'zh-cn',\n",
    "                                              \"title\": title},\n",
    "                            \n",
    "                        })\n",
    "\n",
    "                        count += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25132351",
   "metadata": {},
   "outputs": [],
   "source": [
    "#åŒæ—¶ä¿å­˜rawå’Œcleanedæ–¹ä¾¿åç»­å¯¹æ¯”\n",
    "cc = OpenCC('t2s')  # ç¹è½¬ç®€\n",
    "\n",
    "def processing_and_save(input_dir, output_jsonl_path, output_jsonl_path2, max_docs=1000):\n",
    "    count = 0\n",
    "    with jsonlines.open(output_jsonl_path, 'w') as cleaned_writer, \\\n",
    "         jsonlines.open(output_jsonl_path2, 'w') as raw_writer:\n",
    "\n",
    "        for dirpath, _, filenames in os.walk(input_dir):\n",
    "            filenames.sort()\n",
    "            for fname in filenames:\n",
    "                if count >= max_docs:\n",
    "                    return\n",
    "                path = os.path.join(dirpath, fname)\n",
    "                with open(path, 'r', encoding='utf-8') as f:\n",
    "                    for line in f:\n",
    "                        if count >= max_docs:\n",
    "                            return\n",
    "                        data = json.loads(line)\n",
    "                        raw_text = data['text']\n",
    "                        title = cc.convert(data.get('title', ''))\n",
    "                        id_ = data.get('id', '')\n",
    "\n",
    "                        # 1. ä¿å­˜åŸå§‹æ•°æ®\n",
    "                        raw_writer.write({\n",
    "                            \"text\": raw_text.strip(),\n",
    "                            \"meta\": {\n",
    "                                \"id\": id_,\n",
    "                                \"source\": \"zhwiki\",\n",
    "                                \"language\": \"zh-cn\",\n",
    "                                \"title\": title\n",
    "                            }\n",
    "                        })\n",
    "                        count += 1  # æ³¨æ„ï¼šåªä»¥åŸå§‹ä¸ºåŸºå‡†\n",
    "\n",
    "                        # 2. æ¸…æ´—å¹¶ä¿å­˜æ¸…æ´—åçš„ï¼ˆå¯èƒ½è·³è¿‡ï¼‰\n",
    "                        cleaned = clean_text(raw_text)\n",
    "                        cleaned = clean_text_2(cleaned)\n",
    "                        if not is_valid_text(cleaned):\n",
    "                            continue\n",
    "\n",
    "                        cleaned_writer.write({\n",
    "                            \"text\": cleaned.strip(),\n",
    "                            \"meta\": {\n",
    "                                \"id\": id_,\n",
    "                                \"source\": \"zhwiki\",\n",
    "                                \"language\": \"zh-cn\",\n",
    "                                \"title\": title\n",
    "                            }\n",
    "                        })"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6f76cea",
   "metadata": {},
   "outputs": [],
   "source": [
    "n =20000\n",
    "processing_and_save(input_dir='extracted', output_jsonl_path=f'cleaned_samples_{n}.jsonl',output_jsonl_path2=f'raw_samples_{n}.jsonl', max_docs=n)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d1fa8a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "#ç»˜å›¾åˆ†æå¤„ç†å‰åå†…å®¹é•¿åº¦\n",
    "import json\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# æ–‡ä»¶è·¯å¾„\n",
    "cleaned_path = f\"cleaned_samples_{n}.jsonl\"\n",
    "raw_path = f\"raw_samples_{n}.jsonl\"\n",
    "\n",
    "# ç»Ÿè®¡æ¯æ¡ text çš„é•¿åº¦\n",
    "def get_text_lengths(filepath):\n",
    "    lengths = []\n",
    "    with open(filepath, 'r', encoding='utf-8') as f:\n",
    "        for line in f:\n",
    "            obj = json.loads(line)\n",
    "            text = obj.get(\"text\", \"\")\n",
    "            lengths.append(len(text))\n",
    "    return lengths\n",
    "\n",
    "cleaned_lengths = get_text_lengths(cleaned_path)\n",
    "raw_lengths = get_text_lengths(raw_path)\n",
    "\n",
    "def plot_trimmed_hist( data, label, bins=40, logy=False, q_range=(0.0, 0.99)):\n",
    "    # 1. è®¡ç®—ç™¾åˆ†ä½æ•°èŒƒå›´\n",
    "    lower, upper = np.quantile(data, q_range)\n",
    "\n",
    "    # 2. ä»…ç»˜åˆ¶ä½äºä¸­é—´90%åŒºé—´çš„æ•°æ®\n",
    "    trimmed = [x for x in data if lower <= x <= upper]\n",
    "    \n",
    "    # 3. ç”»å›¾\n",
    "    plt.hist(trimmed, bins=bins, density=True,alpha=0.6,label=label)\n",
    "\n",
    "# ç”»å›¾\n",
    "plt.figure(figsize=(8, 6))\n",
    "#plot_trimmed_hist(raw_lengths, bins=40,label=\"Raw\",q_range=(0.0, 0.99))\n",
    "#plot_trimmed_hist(cleaned_lengths, bins=40,label=\"Cleaned\",q_range=(0.0, 0.95))\n",
    "plt.hist(raw_lengths, bins=400, alpha=0.6, label=\"Raw\", color='blue', edgecolor='black')\n",
    "plt.hist(cleaned_lengths, bins=400, alpha=0.6, label=\"Cleaned\", color='green', edgecolor='black')\n",
    "plt.xlabel(\"Text Length\")\n",
    "plt.xlim(0,5000)\n",
    "plt.yscale('log')\n",
    "plt.ylabel(\"Count\")\n",
    "plt.title(\"Distribution of Text Lengths: Raw vs Cleaned\")\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12c835bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ä½¿ç”¨æ ‡å‡† json è¯»å– jsonl æ–‡ä»¶æ›¿ä»£ jsonlines\n",
    "import json\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import re\n",
    "from collections import Counter\n",
    "\n",
    "# è¯»å–æ–‡ä»¶è·¯å¾„\n",
    "cleaned_path = f\"cleaned_samples_{n}.jsonl\"\n",
    "\n",
    "# å­˜å‚¨å„ç±»ç»Ÿè®¡å€¼\n",
    "content_lengths = []\n",
    "line_numbers = []\n",
    "token_lengths = []\n",
    "non_alpha_fractions = []\n",
    "unique_words_fractions = []\n",
    "mean_word_lengths = []\n",
    "sentence_numbers = []\n",
    "stop_word_fractions = []\n",
    "symbol_to_word_ratios = []\n",
    "\n",
    "# ç®€å•çš„ä¸­æ–‡åœç”¨è¯è¡¨ï¼ˆå¯æ ¹æ®éœ€è¦æ‰©å±•ï¼‰\n",
    "stop_words = set(\"çš„äº†æ˜¯åœ¨å’Œæ˜¯ä¹Ÿå°±éƒ½è€ŒåŠä¸\".split())\n",
    "\n",
    "# åˆ†å¥æ­£åˆ™\n",
    "sentence_splitter = re.compile(r'[ã€‚ï¼ï¼Ÿ!?ï¼›;]+')\n",
    "\n",
    "with open(cleaned_path, 'r', encoding='utf-8') as f:\n",
    "    for line in f:\n",
    "        obj = json.loads(line)\n",
    "        text = obj[\"text\"]\n",
    "        content_lengths.append(len(text))\n",
    "\n",
    "        lines = text.splitlines()\n",
    "        line_numbers.append(len(lines))\n",
    "\n",
    "        tokens = list(text)\n",
    "        token_lengths.append(len(tokens))\n",
    "\n",
    "        num_alpha = len([c for c in text if c.isalpha()])\n",
    "        non_alpha_fractions.append((len(text) - num_alpha) / len(text))\n",
    "\n",
    "        words = list(text)\n",
    "        word_count = Counter(words)\n",
    "        unique_words_fractions.append(len(word_count) / len(words))\n",
    "\n",
    "        mean_word_lengths.append(np.mean([1 if ord(w) > 255 else 2 for w in words]))  # ç²—ç•¥ä¼°è®¡ï¼šè‹±æ–‡ 2ï¼Œä¸­æ–‡ 1\n",
    "\n",
    "        sentences = sentence_splitter.split(text)\n",
    "        sentence_numbers.append(len([s for s in sentences if s.strip()]))\n",
    "\n",
    "        stop_word_count = sum([1 for w in words if w in stop_words])\n",
    "        stop_word_fractions.append(stop_word_count / len(words))\n",
    "\n",
    "        symbol_count = len([c for c in text if not c.isalnum()])\n",
    "        symbol_to_word_ratios.append(symbol_count / len(words))\n",
    "\n",
    "# å¯è§†åŒ–\n",
    "fig, axes = plt.subplots(2, 3, figsize=(12, 7))\n",
    "axes = axes.flatten()\n",
    "metrics = [\n",
    "    (content_lengths, '(a) Content Length'),\n",
    "    (line_numbers, '(b) Line Number'),\n",
    "    (sentence_numbers, '(g) Sentence Number'),\n",
    "    (non_alpha_fractions, '(d) Non-alpha Fraction'),\n",
    "    (unique_words_fractions, '(e) Unique Words Fraction'),\n",
    "    (symbol_to_word_ratios, '(i) Symbol to Word Ratio')\n",
    "    \n",
    "]\n",
    "\n",
    "def plot_trimmed_hist(ax, data, label, bins=40, logy=False, q_range=(0.0, 0.95)):\n",
    "    # 1. è®¡ç®—ç™¾åˆ†ä½æ•°èŒƒå›´\n",
    "    lower, upper = np.quantile(data, q_range)\n",
    "\n",
    "    # 2. ä»…ç»˜åˆ¶ä½äºä¸­é—´90%åŒºé—´çš„æ•°æ®\n",
    "    trimmed = [x for x in data if lower <= x <= upper]\n",
    "    \n",
    "    # 3. ç”»å›¾\n",
    "    ax.hist(trimmed, bins=bins, density=True)\n",
    "    ax.set_title(label)\n",
    "    if logy:\n",
    "        ax.set_yscale('log')\n",
    "\n",
    "for i, (data, label) in enumerate(metrics):\n",
    "    if i <3:\n",
    "        plot_trimmed_hist(axes[i], content_lengths, \"(a) Content Length\", logy=True)\n",
    "    else:\n",
    "        axes[i].hist(data, bins=40, density=True)\n",
    "        axes[i].set_title(label)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c7ea0566",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "wiki",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
