{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "4a1368fd",
   "metadata": {},
   "source": [
    "安装依赖\n",
    "\n",
    "conda create -n wiki python=3.10\n",
    "\n",
    "conda activate wiki\n",
    "\n",
    "pip install -r requirements.txt"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe357f0b",
   "metadata": {},
   "source": [
    "# 提取数据"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "131d4fe9",
   "metadata": {},
   "source": [
    "### wikiextractor的好处\n",
    "- 自动识别并清除语法结构（引用；模板、文件、HTML标签、表格、图像引用等），只保留正文内容\n",
    "- 分段合理，保留文章结构，每篇文章会被解析为：title，text，并按段落分块，便于后续使用或精调。\n",
    "- 支持大规模并行\n",
    "- 多语言支持\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa724d79",
   "metadata": {},
   "outputs": [],
   "source": [
    "!wikiextractor ../zhwiki-20250601-pages-articles-multistream.xml.bz2 -o extracted --json"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f513939",
   "metadata": {},
   "source": [
    "# 启发式过滤\n",
    "\n",
    "为了提升语料质量，我们在预处理后的文本上应用了一系列启发式规则，主要针对维基数据中的冗余、无效或格式不规范内容进行过滤和清洗。以下是具体策略：\n",
    "1. 长度过滤(太短可能信息不足)\n",
    "2. 重定向过滤\n",
    "3. 语言转换（处理 Wikipedia 的多语言模板 -{zh-cn:...}-，以及利用opencc库繁体换简体）\n",
    "4. 结构冗余清理（清除空括号 (), （，）, （，）等格式残留或错误）\n",
    "5. 无意义标题过滤（删除标题/列表类短句：若该行为标题式内容（字数≤ 15 不构成完整句子），认为是结构噪声，删除该行）\n",
    "6. 英文比例过滤（对于英文字符明显超过中文字符的句子，可跳过保留中文主干为主）\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3106ce4b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# clean_and_convert.py\n",
    "import os\n",
    "import json\n",
    "import jsonlines\n",
    "import re\n",
    "from tqdm import tqdm\n",
    "from opencc import OpenCC\n",
    "\n",
    "cc = OpenCC('t2s')\n",
    "\n",
    "def is_valid_text(text):\n",
    "    \"\"\"过滤无效内容（太短、含特殊词、重定向）\"\"\"\n",
    "    if len(text.strip()) < 200 or len(text.strip()) >8000:\n",
    "        return False\n",
    "    if '#REDIRECT' in text or '#重定向' in text:\n",
    "        return False\n",
    "    return True\n",
    "\n",
    "def clean_text(text):\n",
    "    \"\"\"简单清洗模板/引用/分类\"\"\"\n",
    "    #text = re.sub(r'\\[\\[Category:[^\\]]+\\]\\]', '', text)  # 删除分类\n",
    "    #text = re.sub(r'<ref[^<]*</ref>', '', text)          # 删除ref引用\n",
    "    #text = re.sub(r'{{[^{}]+}}', '', text)               # 删除模板\n",
    "    #text = re.sub(r'==+[^=]+==+', '', text)              # 删除标题\n",
    "    text = re.sub(r'\\n+', '\\n', text).strip()            # 多个换行合并\n",
    "    text = cc.convert(text)\n",
    "    return text\n",
    "\n",
    "\n",
    "def remove_bad_parentheses(text):\n",
    "    def should_remove(content):\n",
    "        # 超过一半是英文字母或特殊符号\n",
    "        english = len(re.findall(r'[A-Za-z]', content))\n",
    "        chinese = len(re.findall(r'[\\u4e00-\\u9fff]', content))\n",
    "        symbols = len(re.findall(r'[\\W_]', content))  # 非字母数字\n",
    "        total = max(len(content), 1)\n",
    "\n",
    "        english_ratio = english / total\n",
    "        symbol_ratio = symbols / total\n",
    "\n",
    "        # 明显是拉丁文名或特殊语种\n",
    "        bad_keywords = ['学名', '拉丁', '英文', '英语', '德语', 'Latin', '名称','旧称','译名','港译','又译','日语','日文']\n",
    "\n",
    "        if english_ratio > 0.7 and chinese == 0:\n",
    "            return True\n",
    "        if symbol_ratio > 0.4:\n",
    "            return True\n",
    "        if any(kw in content for kw in bad_keywords):\n",
    "            return True\n",
    "        return False\n",
    "\n",
    "    # 匹配中英文括号对\n",
    "    return re.sub(\n",
    "        r'（([^（）]{0,30})）|[(（]([^()（）]{0,30})[)）]',\n",
    "        lambda m: (\n",
    "            '' if should_remove((m.group(1) or '') + (m.group(2) or '')) else m.group(0)\n",
    "        ),\n",
    "        text\n",
    "    )\n",
    "\n",
    "def clean_text_2(text: str) -> str:\n",
    "    # 1. 处理语言选择语法：保留 zh-cn 部分\n",
    "    text = re.sub(r'-\\{[^{}]*?zh-cn:([^;{}]+?)(;[^{}]*)?\\}-', r'\\1', text)\n",
    "    text = re.sub(r'-\\{[^{}]*?zh-hans:([^;{}]+?)(;[^{}]*)?\\}-', r'\\1', text)\n",
    "\n",
    "    # 2. 括号内英文拉丁乱码处理\n",
    "    text = remove_bad_parentheses(text)\n",
    "    # 去掉括号内语言标注类内容，如“德语: xxx”、“英语: xxx”、“法语: xxx”\n",
    "    text = re.sub(r'[（(](德语|英语|法语|俄语|日语|韩语|西班牙语|拉丁语|荷兰语|捷克语|土耳其语|意大利语|葡萄牙语|匈牙利语|芬兰语|乌克兰语|保加利亚语|希腊语|瑞典语|丹麦语|挪威语|罗马尼亚语|斯洛文尼亚语|爱尔兰语|波兰语|阿拉伯语|希伯来语|世界语)[：:][^）)]{1,50}[）)]', '', text)\n",
    "    # 去除括号内以标点开头的内容（如\"（，缩写：EUVE）\"）\n",
    "    text = re.sub(r'[（(][，、。、：:；;\\s\\'\"‘’“”`~^!@#$%^&*，.?!\\-+=<>…·╯￣_＝×]{1}[^（）()]{0,50}[）)]', '', text)\n",
    "\n",
    "    text = re.sub(r'\\((\\s*[，、,]?\\s*)\\)', '', text)\n",
    "    text = re.sub(r'（\\s*[，、,]?\\s*）', '', text)\n",
    "\n",
    "    # 3. 删除无意义的短标题行（≤ 5字，且无标点）\n",
    "    cleaned_lines = []\n",
    "    for line in text.splitlines():\n",
    "        stripped = line.strip()\n",
    "        if len(stripped) <= 15:\n",
    "            continue  # 跳过这类短标题行\n",
    "        cleaned_lines.append(line)\n",
    "    text = \"\\n\".join(cleaned_lines)\n",
    "\n",
    "    return text.strip()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a6574cce",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "def is_mostly_english(line: str, threshold: float = 0.7) -> bool:\n",
    "    english_chars = len(re.findall(r'[A-Za-z]', line))\n",
    "    chinese_chars = len(re.findall(r'[\\u4e00-\\u9fff]', line))\n",
    "    total = len(line.strip())\n",
    "\n",
    "    if total == 0:\n",
    "        return False\n",
    "\n",
    "    ratio = english_chars / (total + 1e-5)\n",
    "    if english_chars > chinese_chars * 2 or ratio > threshold:\n",
    "        return True\n",
    "    return False\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a5868c5",
   "metadata": {},
   "source": [
    "### 过滤可视化"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d33093bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "\n",
    "def inspect_cleaning(input_dir, max_docs=5):\n",
    "    count = 0\n",
    "\n",
    "    for dirpath, _, filenames in os.walk(input_dir):\n",
    "        print(filenames.sort())\n",
    "        for fname in filenames:\n",
    "            if count >= max_docs:\n",
    "                return\n",
    "            path = os.path.join(dirpath, fname)\n",
    "            \n",
    "            with open(path, 'r', encoding='utf-8') as f:\n",
    "                for line in f:\n",
    "                    if count >= max_docs:\n",
    "                        return\n",
    "                    data = json.loads(line)\n",
    "                    raw_text = data['text']\n",
    "                    title = data['title']\n",
    "                    id = data['id']\n",
    "                    cleaned_text = clean_text(raw_text)\n",
    "                    cleaned_text = clean_text_2(cleaned_text)\n",
    "\n",
    "                    print(\"=\" * 80)\n",
    "                    print(f\"📄 文件名: {fname}\")\n",
    "                    print(title,id)\n",
    "                    print(f\"🧾 原始文本前 300 字:\\n{raw_text[:]}\")\n",
    "                    print(\"-\" * 80)\n",
    "                    print(f\"✅ 清洗后文本前 300 字:\\n{cleaned_text[:]}\")\n",
    "                    print(\"=\" * 80)\n",
    "\n",
    "                    #input(\"🔍 按 Enter 查看下一条，或 Ctrl+C 中止...\")\n",
    "                    count += 1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a22614d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "inspect_cleaning(\"extracted/AA/\", max_docs=5)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc5ed123",
   "metadata": {},
   "source": [
    "### 保存jsonl文件"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a124b7c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def processing_and_save(input_dir, output_jsonl_path, max_docs=1000):\n",
    "    count = 0\n",
    "    with jsonlines.open(output_jsonl_path, 'w') as writer:\n",
    "        for dirpath, _, filenames in os.walk(input_dir):\n",
    "            filenames.sort()\n",
    "            for fname in filenames:\n",
    "                if count >= max_docs:\n",
    "                    return\n",
    "                path = os.path.join(dirpath, fname)\n",
    "                with open(path, 'r', encoding='utf-8') as f:\n",
    "                    for line in f:\n",
    "                        if count >= max_docs:\n",
    "                            return\n",
    "                        data = json.loads(line)\n",
    "                        raw_text = data['text']\n",
    "                        title = cc.convert(data.get('title', ''))\n",
    "                        id_ = data.get('id', '')\n",
    "\n",
    "                        cleaned = clean_text(raw_text)\n",
    "                        cleaned = clean_text_2(cleaned)\n",
    "                        if not is_valid_text(cleaned):\n",
    "                            continue\n",
    "\n",
    "                        # 保存为对比 jsonl 文件\n",
    "                        writer.write({\"text\":cleaned.strip(),\n",
    "                            'meta':{\"id\": id_,\n",
    "                                              \"source\": \"zhwiki\",\n",
    "                                              'language':'zh-cn',\n",
    "                                              \"title\": title},\n",
    "                            \n",
    "                        })\n",
    "\n",
    "                        count += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25132351",
   "metadata": {},
   "outputs": [],
   "source": [
    "#同时保存raw和cleaned方便后续对比\n",
    "cc = OpenCC('t2s')  # 繁转简\n",
    "\n",
    "def processing_and_save(input_dir, output_jsonl_path, output_jsonl_path2, max_docs=1000):\n",
    "    count = 0\n",
    "    with jsonlines.open(output_jsonl_path, 'w') as cleaned_writer, \\\n",
    "         jsonlines.open(output_jsonl_path2, 'w') as raw_writer:\n",
    "\n",
    "        for dirpath, _, filenames in os.walk(input_dir):\n",
    "            filenames.sort()\n",
    "            for fname in filenames:\n",
    "                if count >= max_docs:\n",
    "                    return\n",
    "                path = os.path.join(dirpath, fname)\n",
    "                with open(path, 'r', encoding='utf-8') as f:\n",
    "                    for line in f:\n",
    "                        if count >= max_docs:\n",
    "                            return\n",
    "                        data = json.loads(line)\n",
    "                        raw_text = data['text']\n",
    "                        title = cc.convert(data.get('title', ''))\n",
    "                        id_ = data.get('id', '')\n",
    "\n",
    "                        # 1. 保存原始数据\n",
    "                        raw_writer.write({\n",
    "                            \"text\": raw_text.strip(),\n",
    "                            \"meta\": {\n",
    "                                \"id\": id_,\n",
    "                                \"source\": \"zhwiki\",\n",
    "                                \"language\": \"zh-cn\",\n",
    "                                \"title\": title\n",
    "                            }\n",
    "                        })\n",
    "                        count += 1  # 注意：只以原始为基准\n",
    "\n",
    "                        # 2. 清洗并保存清洗后的（可能跳过）\n",
    "                        cleaned = clean_text(raw_text)\n",
    "                        cleaned = clean_text_2(cleaned)\n",
    "                        if not is_valid_text(cleaned):\n",
    "                            continue\n",
    "\n",
    "                        cleaned_writer.write({\n",
    "                            \"text\": cleaned.strip(),\n",
    "                            \"meta\": {\n",
    "                                \"id\": id_,\n",
    "                                \"source\": \"zhwiki\",\n",
    "                                \"language\": \"zh-cn\",\n",
    "                                \"title\": title\n",
    "                            }\n",
    "                        })"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6f76cea",
   "metadata": {},
   "outputs": [],
   "source": [
    "n =20000\n",
    "processing_and_save(input_dir='extracted', output_jsonl_path=f'cleaned_samples_{n}.jsonl',output_jsonl_path2=f'raw_samples_{n}.jsonl', max_docs=n)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d1fa8a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "#绘图分析处理前后内容长度\n",
    "import json\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# 文件路径\n",
    "cleaned_path = f\"cleaned_samples_{n}.jsonl\"\n",
    "raw_path = f\"raw_samples_{n}.jsonl\"\n",
    "\n",
    "# 统计每条 text 的长度\n",
    "def get_text_lengths(filepath):\n",
    "    lengths = []\n",
    "    with open(filepath, 'r', encoding='utf-8') as f:\n",
    "        for line in f:\n",
    "            obj = json.loads(line)\n",
    "            text = obj.get(\"text\", \"\")\n",
    "            lengths.append(len(text))\n",
    "    return lengths\n",
    "\n",
    "cleaned_lengths = get_text_lengths(cleaned_path)\n",
    "raw_lengths = get_text_lengths(raw_path)\n",
    "\n",
    "def plot_trimmed_hist( data, label, bins=40, logy=False, q_range=(0.0, 0.99)):\n",
    "    # 1. 计算百分位数范围\n",
    "    lower, upper = np.quantile(data, q_range)\n",
    "\n",
    "    # 2. 仅绘制位于中间90%区间的数据\n",
    "    trimmed = [x for x in data if lower <= x <= upper]\n",
    "    \n",
    "    # 3. 画图\n",
    "    plt.hist(trimmed, bins=bins, density=True,alpha=0.6,label=label)\n",
    "\n",
    "# 画图\n",
    "plt.figure(figsize=(8, 6))\n",
    "#plot_trimmed_hist(raw_lengths, bins=40,label=\"Raw\",q_range=(0.0, 0.99))\n",
    "#plot_trimmed_hist(cleaned_lengths, bins=40,label=\"Cleaned\",q_range=(0.0, 0.95))\n",
    "plt.hist(raw_lengths, bins=400, alpha=0.6, label=\"Raw\", color='blue', edgecolor='black')\n",
    "plt.hist(cleaned_lengths, bins=400, alpha=0.6, label=\"Cleaned\", color='green', edgecolor='black')\n",
    "plt.xlabel(\"Text Length\")\n",
    "plt.xlim(0,5000)\n",
    "plt.yscale('log')\n",
    "plt.ylabel(\"Count\")\n",
    "plt.title(\"Distribution of Text Lengths: Raw vs Cleaned\")\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12c835bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 使用标准 json 读取 jsonl 文件替代 jsonlines\n",
    "import json\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import re\n",
    "from collections import Counter\n",
    "\n",
    "# 读取文件路径\n",
    "cleaned_path = f\"cleaned_samples_{n}.jsonl\"\n",
    "\n",
    "# 存储各类统计值\n",
    "content_lengths = []\n",
    "line_numbers = []\n",
    "token_lengths = []\n",
    "non_alpha_fractions = []\n",
    "unique_words_fractions = []\n",
    "mean_word_lengths = []\n",
    "sentence_numbers = []\n",
    "stop_word_fractions = []\n",
    "symbol_to_word_ratios = []\n",
    "\n",
    "# 简单的中文停用词表（可根据需要扩展）\n",
    "stop_words = set(\"的了是在和是也就都而及与\".split())\n",
    "\n",
    "# 分句正则\n",
    "sentence_splitter = re.compile(r'[。！？!?；;]+')\n",
    "\n",
    "with open(cleaned_path, 'r', encoding='utf-8') as f:\n",
    "    for line in f:\n",
    "        obj = json.loads(line)\n",
    "        text = obj[\"text\"]\n",
    "        content_lengths.append(len(text))\n",
    "\n",
    "        lines = text.splitlines()\n",
    "        line_numbers.append(len(lines))\n",
    "\n",
    "        tokens = list(text)\n",
    "        token_lengths.append(len(tokens))\n",
    "\n",
    "        num_alpha = len([c for c in text if c.isalpha()])\n",
    "        non_alpha_fractions.append((len(text) - num_alpha) / len(text))\n",
    "\n",
    "        words = list(text)\n",
    "        word_count = Counter(words)\n",
    "        unique_words_fractions.append(len(word_count) / len(words))\n",
    "\n",
    "        mean_word_lengths.append(np.mean([1 if ord(w) > 255 else 2 for w in words]))  # 粗略估计：英文 2，中文 1\n",
    "\n",
    "        sentences = sentence_splitter.split(text)\n",
    "        sentence_numbers.append(len([s for s in sentences if s.strip()]))\n",
    "\n",
    "        stop_word_count = sum([1 for w in words if w in stop_words])\n",
    "        stop_word_fractions.append(stop_word_count / len(words))\n",
    "\n",
    "        symbol_count = len([c for c in text if not c.isalnum()])\n",
    "        symbol_to_word_ratios.append(symbol_count / len(words))\n",
    "\n",
    "# 可视化\n",
    "fig, axes = plt.subplots(2, 3, figsize=(12, 7))\n",
    "axes = axes.flatten()\n",
    "metrics = [\n",
    "    (content_lengths, '(a) Content Length'),\n",
    "    (line_numbers, '(b) Line Number'),\n",
    "    (sentence_numbers, '(g) Sentence Number'),\n",
    "    (non_alpha_fractions, '(d) Non-alpha Fraction'),\n",
    "    (unique_words_fractions, '(e) Unique Words Fraction'),\n",
    "    (symbol_to_word_ratios, '(i) Symbol to Word Ratio')\n",
    "    \n",
    "]\n",
    "\n",
    "def plot_trimmed_hist(ax, data, label, bins=40, logy=False, q_range=(0.0, 0.95)):\n",
    "    # 1. 计算百分位数范围\n",
    "    lower, upper = np.quantile(data, q_range)\n",
    "\n",
    "    # 2. 仅绘制位于中间90%区间的数据\n",
    "    trimmed = [x for x in data if lower <= x <= upper]\n",
    "    \n",
    "    # 3. 画图\n",
    "    ax.hist(trimmed, bins=bins, density=True)\n",
    "    ax.set_title(label)\n",
    "    if logy:\n",
    "        ax.set_yscale('log')\n",
    "\n",
    "for i, (data, label) in enumerate(metrics):\n",
    "    if i <3:\n",
    "        plot_trimmed_hist(axes[i], content_lengths, \"(a) Content Length\", logy=True)\n",
    "    else:\n",
    "        axes[i].hist(data, bins=40, density=True)\n",
    "        axes[i].set_title(label)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c7ea0566",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "wiki",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
